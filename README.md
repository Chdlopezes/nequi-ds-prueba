# üß† Detecci√≥n de Fraccionamiento Transaccional ‚Äì Prueba T√©cnica NEQUI

Este proyecto aborda el reto de **identificar malas pr√°cticas transaccionales**, en particular el **fraccionamiento de transacciones**, donde un usuario divide una operaci√≥n grande en varias m√°s peque√±as dentro de un periodo corto de tiempo.  
El objetivo es dise√±ar un **producto de datos** que permita detectar, analizar y actualizar este tipo de comportamientos de manera sistem√°tica y escalable.

---

## üìÇ Estructura general del proyecto

El proyecto est√° dividido en **dos etapas principales**:

1. **An√°lisis descriptivo y exploratorio (EDA)**  
   Implementado mediante notebooks de an√°lisis que permiten comprender los datos, identificar patrones y generar las primeras etiquetas de comportamiento fraudulento.

2. **Modelo operativo simulado (ETL + Backend)**  
   Un prototipo funcional desarrollado en `Flask` que simula la actualizaci√≥n diaria de transacciones, detecci√≥n de fraccionamientos y actualizaci√≥n de tablas agregadas en una base de datos relacional (SQLite en entorno local).

---

## üß© Estructura del repositorio

‚îú‚îÄ‚îÄ app.py
‚îú‚îÄ‚îÄ data
‚îÇ ‚îú‚îÄ‚îÄ interim
‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ fractioned_transactions.parquet
‚îÇ ‚îú‚îÄ‚îÄ sample_data_0006_part_00.parquet
‚îÇ ‚îú‚îÄ‚îÄ sample_data_0007_part_00.parquet
‚îÇ ‚îî‚îÄ‚îÄ utils
‚îÇ ‚îî‚îÄ‚îÄ possible_transaction_thresholds.csv
‚îú‚îÄ‚îÄ database.py
‚îú‚îÄ‚îÄ instance
‚îÇ ‚îî‚îÄ‚îÄ test.db
‚îú‚îÄ‚îÄ models.py
‚îú‚îÄ‚îÄ notebooks
‚îÇ ‚îú‚îÄ‚îÄ description.ipynb
‚îÇ ‚îî‚îÄ‚îÄ exploration.ipynb
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ start.py
‚îî‚îÄ‚îÄ utils.py

---

## üßÆ 1. An√°lisis descriptivo (`description.ipynb`)

Este notebook realiza una exploraci√≥n profunda del conjunto de datos **`sample_data_006_part_00.parquet`**, utilizando la librer√≠a **DuckDB**, ideal para trabajar con grandes vol√∫menes sin cargarlos completamente en memoria.

Los pasos principales incluyen:

- Exploraci√≥n general (tama√±o, tipos de variables, valores nulos, rangos de fechas, valores √∫nicos, outliers).
- An√°lisis de patrones transaccionales por usuario, cuenta y comercio.
- Identificaci√≥n de posibles **transacciones fraccionadas**, definidas como transacciones de un mismo usuario al mismo comercio dentro del **mismo d√≠a calendario**.
  > Esta decisi√≥n evita considerar como fraccionadas transacciones hechas, por ejemplo, a las 6 p.m. y 4 p.m. del d√≠a siguiente.

El resultado de este an√°lisis genera un archivo intermedio:

`fractioned_transactions.parquet`

Este archivo contiene:

- `_id`: identificador √∫nico de la transacci√≥n.
- `label`: identificador com√∫n para transacciones que pertenecen a una misma transacci√≥n original fraccionada.

---

## üìä 2. An√°lisis exploratorio (`exploratory.ipynb`)

En esta segunda etapa se profundiza en el an√°lisis de las **transacciones fraccionadas** y su relaci√≥n con factores clave, tales como:

- Dispersi√≥n en los montos de transacci√≥n.
- Cantidad de transacciones fraccionadas por comercio y por cuenta.
- Posibles l√≠mites de monto establecidos por cada comercio o cuenta.
- Distribuci√≥n y correlaciones de variables relevantes.

El notebook produce an√°lisis estad√≠sticos agregados por **comercio**, **sucursal** y **n√∫mero de cuenta**, permitiendo detectar patrones estructurales de comportamiento y relaciones con l√≠mites transaccionales.

---

## ‚öôÔ∏è 3. Backend y modelo operativo simulado (Flask)

Para ilustrar c√≥mo este producto de datos podr√≠a integrarse en un entorno operativo, se desarroll√≥ un **modelo de backend** usando el framework **Flask**.

### üîÅ Proceso ETL

El endpoint `/update_transactions` realiza el proceso de **Extracci√≥n, Transformaci√≥n y Carga (ETL)**:

1. Lee nuevas transacciones (por ejemplo desde `sample_data_007_part_00.parquet`).
2. Actualiza las tablas en la base de datos local (SQLite).
3. Identifica nuevas transacciones fraccionadas y actualiza los registros correspondientes.
4. Genera tablas materializadas agregadas por comercio o cuenta, actualizadas **cada 24 horas**.

Las tablas manejadas actualmente son:

- `transactions` ‚Äì registro completo de transacciones originales.
- `fractioned_transactions` ‚Äì transacciones identificadas como fraccionadas, vinculadas mediante `_id` (foreign key).

> En un entorno productivo, este esquema podr√≠a migrarse f√°cilmente a PostgreSQL o MySQL, y los archivos procesados a un _data lake_ en la nube (AWS S3, GCP Storage, etc.).

---

## üßæ 4. Configuraci√≥n y ejecuci√≥n

### üß∞ Requisitos previos

Instalar las dependencias necesarias:

````bash
pip install -r requirements.txt


Aseg√∫rate de colocar los archivos de datos originales en el directorio data/:

data/
 ‚îú‚îÄ‚îÄ sample_data_006_part_00.parquet
 ‚îî‚îÄ‚îÄ sample_data_007_part_00.parquet


### üöÄ Configuracion inicial
Para crear la base de datos local y poblarla con datos de ejemplo:

```bash
python start.py
````

Esto generar√° un archivo SQLite local con las tablas iniciales.

Ejecutar el servidor Flask:

```bash
python app.py
```

El servicio se ejecutar√° en el puerto 5000 por defecto.
El endpoint principal /update_transactions puede ser accedido v√≠a GET:

```bash
GET http://localhost:5000/update_transactions
```

Este endpoint actualizar√° las tablas y generar√° los archivos procesados en:

`data/processed/`

üìà 5. Resultados y actualizaciones

Las m√©tricas, distribuciones y correlaciones se recalculan autom√°ticamente con cada actualizaci√≥n.

El sistema permite mantener un flujo continuo de retroalimentaci√≥n diaria, fortaleciendo la detecci√≥n de nuevas pr√°cticas de fraccionamiento.

No se incorporan modelos predictivos en esta fase inicial, dado que el enfoque no lo hace necesario, siendo el proceso de identificaci√≥n basado en reglas claras y definidas. Se agrega valor mediante la automatizaci√≥n y estructuraci√≥n del proceso.

üß© 6. Pasos para despliegue en produccion

Mejorar el pipeline ETL para fuentes externas (API o mensajer√≠a).

Implementar almacenamiento y visualizaci√≥n en dashboards (Power BI, Grafana o Streamlit).

Migraci√≥n del motor de base de datos a PostgreSQL con tareas automatizadas (Airflow o Prefect).

Contenerizaci√≥n con Docker para despliegue en la nube (AWS, GCP o Azure). Con orquestaci√≥n con Kubernetes.
